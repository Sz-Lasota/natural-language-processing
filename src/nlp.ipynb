{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:31:01.212440Z",
     "start_time": "2025-12-05T11:31:01.209548Z"
    },
    "id": "heHwQb5io9IH"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:31:08.034991Z",
     "start_time": "2025-12-05T11:31:06.104923Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "LehkNYcSmX16",
    "outputId": "43796e6b-c8ae-452a-fc9b-e3b0d62afebb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/szymon/.cache/kagglehub/datasets/abhi8923shriv/sentiment-analysis-dataset/versions/9/train.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/szymon/.cache/kagglehub/datasets/abhi8923shriv/sentiment-analysis-dataset/versions/9/train.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download dataset from Kaggle\n",
    "ds_path = kagglehub.dataset_download(\"abhi8923shriv/sentiment-analysis-dataset\", path=\"train.csv\")\n",
    "print(\"Path to dataset files:\", ds_path)\n",
    "ds_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWOe67KZpbO_"
   },
   "source": [
    "### Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:31:20.046350Z",
     "start_time": "2025-12-05T11:31:20.042633Z"
    },
    "id": "mnCB7USZpXK4"
   },
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize tweet text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw tweet text\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text with URLs, HTML, mentions removed\n",
    "\n",
    "    Example:\n",
    "        >>> clean_text(\"@user I LOVE this! #amazing\")\n",
    "        'i love this! amazing'\n",
    "\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#(\\w+)\", r\"\\1\", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "\n",
    "def load_train_dataset(\n",
    "    path: str,\n",
    "    encoding: str = \"latin1\",\n",
    "    samples: int = 512,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Load and preprocess the sentiment dataset.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the CSV file\n",
    "        encoding (str): File encoding (default: 'latin1')\n",
    "        samples (int): Number of samples to randomly select\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X, y) where X is text array and y is label array\n",
    "\n",
    "    Example:\n",
    "        >>> X, y = load_train_dataset(\"data.csv\", samples=100)\n",
    "        >>> print(X[0], y[0])\n",
    "        'i love this product' 'positive'\n",
    "\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, encoding=encoding)\n",
    "    df = df.dropna()\n",
    "\n",
    "    df[\"preprocessed_text\"] = df[\"selected_text\"].apply(clean_text)\n",
    "\n",
    "    idxes = random.sample(range(df.shape[0]), samples)\n",
    "    return df[\"preprocessed_text\"].values[idxes], df[\"sentiment\"].values[idxes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUba2yG36sSz"
   },
   "source": [
    "### Prompt Engineering Functions for ICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:31:24.872628Z",
     "start_time": "2025-12-05T11:31:24.869972Z"
    },
    "id": "ev9eCXKp60vJ"
   },
   "outputs": [],
   "source": [
    "def inject_example(text: str, sentiment: str) -> str:\n",
    "    return f\"Tweet: {text}\\nSentiment: {sentiment}\\n\\n\"\n",
    "\n",
    "\n",
    "def inject_sample(text: str) -> str:\n",
    "    return (\n",
    "        f\"### Now classify the following:\\n\"\n",
    "        f\"Tweet: {text}\\nSentiment:\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpQRfB317PO_"
   },
   "source": [
    "### Label Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:31:28.581136Z",
     "start_time": "2025-12-05T11:31:28.578195Z"
    },
    "id": "zh_If7w47TJQ"
   },
   "outputs": [],
   "source": [
    "def extract_label(result: str, labels: list[str]) -> str:\n",
    "\n",
    "    for lab in labels:\n",
    "        if lab in result.lower():\n",
    "            return lab\n",
    "    return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkCeMvXu7wi3"
   },
   "source": [
    "### IN-CONTEXT LEARNING EXPERIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:31:50.573406Z",
     "start_time": "2025-12-05T11:31:50.569358Z"
    },
    "id": "ZMOXfJ4K7ymf"
   },
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    base_prompt: str, X: np.ndarray, y: np.ndarray, model_name: str,\n",
    ") -> tuple[list, list]:\n",
    "    \"\"\"Run ICL classification experiment on a test set.\n",
    "\n",
    "    This function:\n",
    "    1. Loads the specified language model\n",
    "    2. For each test sample, constructs a full prompt\n",
    "    3. Generates a prediction using the model\n",
    "    4. Extracts the predicted label\n",
    "    5. Collects all predictions and ground truth labels\n",
    "\n",
    "    Args:\n",
    "        base_prompt (str): Base prompt with instructions and examples\n",
    "        X (np.ndarray): Array of text samples to classify\n",
    "        y (np.ndarray): Array of true labels\n",
    "        model_name (str): HuggingFace model identifier (e.g., 'facebook/opt-1.3b')\n",
    "\n",
    "    Returns:\n",
    "        tuple[list, list]: (predicted_labels, true_labels)\n",
    "\n",
    "    \"\"\"\n",
    "    labels = list(np.unique(y))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, dtype=torch.float16, device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for idx, x in enumerate(X):\n",
    "        prompt = base_prompt + inject_sample(x)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        pred = result.split()[-1]\n",
    "        pred = extract_label(pred, labels)\n",
    "\n",
    "        y_true.append(y[idx])\n",
    "        y_pred.append(pred)\n",
    "\n",
    "    return y_pred, y_true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYcp5GGZ8ItJ"
   },
   "source": [
    "###  Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "R-6srF-b8Q_s"
   },
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: str, base_prompt: str, X, y, shots_n: int = 3, experiment_func = run_experiment,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Evaluate ICL performance with a specific number of examples (shots).\n",
    "\n",
    "    Workflow:\n",
    "    1. Load (samples + shots_n) data points\n",
    "    2. Use first shots_n samples as examples in the prompt\n",
    "    3. Use remaining samples as the test set\n",
    "    4. Run classification and compute metrics\n",
    "\n",
    "    Args:\n",
    "        model (str): HuggingFace model name\n",
    "        base_prompt (str): Base instruction prompt\n",
    "        path (str): Path to dataset CSV\n",
    "        encoding (str): File encoding\n",
    "        samples (int): Number of test samples\n",
    "        shots_n (int): Number of examples to include (0=zero-shot, 1=one-shot, etc.)\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float, float, float]: (accuracy, f1_score, precision, recall)\n",
    "\n",
    "    \"\"\"\n",
    "    if shots_n > 0:\n",
    "        base_prompt += \"### Examples\\n\"\n",
    "\n",
    "    for i in range(shots_n):\n",
    "        base_prompt += inject_example(X[i], y[i])\n",
    "\n",
    "    X, y = X[shots_n:], y[shots_n:]\n",
    "\n",
    "    y_pred, y_true = experiment_func(base_prompt, X, y, model)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    precision = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return acc, f1, precision, recall, y_true, y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7_C4V158h69"
   },
   "source": [
    "# Eksperymnet ICL konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJrHq2Qy8daE",
    "outputId": "054e2bed-fdd6-46d7-f466-ee88cc19a28a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed repetition 1/10\n",
      "Completed repetition 2/10\n",
      "Completed repetition 2/10\n",
      "Completed repetition 3/10\n",
      "Completed repetition 3/10\n",
      "Completed repetition 4/10\n",
      "Completed repetition 4/10\n",
      "Completed repetition 5/10\n",
      "Completed repetition 5/10\n",
      "Completed repetition 6/10\n",
      "Completed repetition 6/10\n",
      "Completed repetition 7/10\n",
      "Completed repetition 7/10\n",
      "Completed repetition 8/10\n",
      "Completed repetition 8/10\n",
      "Completed repetition 9/10\n",
      "Completed repetition 9/10\n",
      "Completed repetition 10/10\n",
      "Completed repetition 10/10\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Classify the sentiment of the tweet.\n",
    "Choose only one label: positive, negative, neutral.\n",
    "Reply ONLY with the label. Do not explain.\n",
    "\n",
    "\"\"\"\n",
    "shots = [0, 1, 3, 20]\n",
    "reps = 10\n",
    "samples_n = 300\n",
    "result_path = f\"result_{round(time())}.csv\"\n",
    "model_name = \"facebook/opt-2.7b\"\n",
    "safe_name = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "\n",
    "df_prot = {\n",
    "    \"lp\": [],\n",
    "}\n",
    "for shot in shots:\n",
    "    df_prot[f\"{shot}-shots f1\"] = []\n",
    "    df_prot[f\"{shot}-shots acc\"] = []\n",
    "    df_prot[f\"{shot}-shots precision\"] = []\n",
    "    df_prot[f\"{shot}-shots recall\"] = []\n",
    "\n",
    "\n",
    "confusion_matrices = {shot: [] for shot in shots}\n",
    "\n",
    "for i in range(reps):\n",
    "    df_prot[\"lp\"].append(i)\n",
    "    for shot in shots:\n",
    "        X, y = load_train_dataset(\n",
    "            ds_path,\n",
    "            \"latin1\",\n",
    "            samples=samples_n + shot,\n",
    "        )\n",
    "        acc, f1, precision, recall, y_true, y_pred = evaluate(\n",
    "            model_name,\n",
    "            prompt,\n",
    "            X,\n",
    "            y,\n",
    "            shots_n=shot,\n",
    "            experiment_func=run_experiment,\n",
    "        )\n",
    "        df_prot[f\"{shot}-shots f1\"].append(f1)\n",
    "        df_prot[f\"{shot}-shots acc\"].append(acc)\n",
    "        df_prot[f\"{shot}-shots precision\"].append(precision)\n",
    "        df_prot[f\"{shot}-shots recall\"].append(recall)\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[\"positive\", \"negative\", \"neutral\"])\n",
    "        confusion_matrices[shot].append(cm)\n",
    "    print(f\"Completed repetition {i + 1}/{reps}\")\n",
    "\n",
    "df = pd.DataFrame(df_prot)\n",
    "df.to_csv(result_path)\n",
    "\n",
    "with open(f\"../res/shots/results/confusion_matrices_ICL_{safe_name}_{round(time())}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(confusion_matrices, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model size comparasion (with 3-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done model:  facebook/opt-125m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 1/10\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 1/10\n",
      "Done model:  facebook/opt-125m\n",
      "Done model:  facebook/opt-125m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 2/10\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 2/10\n",
      "Done model:  facebook/opt-125m\n",
      "Done model:  facebook/opt-125m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 3/10\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 3/10\n",
      "Done model:  facebook/opt-125m\n",
      "Done model:  facebook/opt-125m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 4/10\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 4/10\n",
      "Done model:  facebook/opt-125m\n",
      "Done model:  facebook/opt-125m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 5/10\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 5/10\n",
      "Done model:  facebook/opt-125m\n",
      "Done model:  facebook/opt-125m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 6/10\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 6/10\n",
      "Done model:  facebook/opt-125m\n",
      "Done model:  facebook/opt-125m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 7/10\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 7/10\n",
      "Done model:  facebook/opt-125m\n",
      "Done model:  facebook/opt-125m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 8/10\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 8/10\n",
      "Done model:  facebook/opt-125m\n",
      "Done model:  facebook/opt-125m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 9/10\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 9/10\n",
      "Done model:  facebook/opt-125m\n",
      "Done model:  facebook/opt-125m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-350m\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-1.3b\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 10/10\n",
      "Done model:  facebook/opt-2.7b\n",
      "Completed repetition 10/10\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Classify the sentiment of the tweet.\n",
    "Choose only one label: positive, negative, neutral.\n",
    "Reply ONLY with the label. Do not explain.\n",
    "\n",
    "\"\"\"\n",
    "shots = 3\n",
    "reps = 10\n",
    "samples_n = 300\n",
    "result_path = f\"../res/models/result_{round(time())}.csv\"\n",
    "\n",
    "if not os.path.exists(\"../res/models/\"):\n",
    "    os.makedirs(\"../res/models/\")\n",
    "\n",
    "models = [\n",
    "    \"facebook/opt-125m\",\n",
    "    \"facebook/opt-350m\",\n",
    "    \"facebook/opt-1.3b\",\n",
    "    \"facebook/opt-2.7b\",\n",
    "    # \"facebook/opt-6.7b\",   # Out of memory on 8GB GPU\n",
    "]\n",
    "safe_name = \"model_size_clf\"\n",
    "\n",
    "\n",
    "df_prot = {\n",
    "    \"lp\": [],\n",
    "}\n",
    "for model in models:\n",
    "    df_prot[f\"{model} f1\"] = []\n",
    "    df_prot[f\"{model} acc\"] = []\n",
    "    df_prot[f\"{model} precision\"] = []\n",
    "    df_prot[f\"{model} recall\"] = []\n",
    "\n",
    "\n",
    "confusion_matrices = {model: [] for model in models}\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "for i in range(reps):\n",
    "    df_prot[\"lp\"].append(i)\n",
    "    X, y = load_train_dataset(\n",
    "        ds_path, \"latin1\", samples=samples_n + shots,\n",
    "    )\n",
    "    for model in models:\n",
    "        acc, f1, precision, recall, y_true, y_pred = evaluate(\n",
    "            model,\n",
    "            prompt,\n",
    "            X,\n",
    "            y,\n",
    "            shots_n=shots,\n",
    "            experiment_func=run_experiment,\n",
    "        )\n",
    "        df_prot[f\"{model} f1\"].append(f1)\n",
    "        df_prot[f\"{model} acc\"].append(acc)\n",
    "        df_prot[f\"{model} precision\"].append(precision)\n",
    "        df_prot[f\"{model} recall\"].append(recall)\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[\"positive\", \"negative\", \"neutral\"])\n",
    "        confusion_matrices[model].append(cm)\n",
    "\n",
    "        # Clearn GPU memory after each model evaluation, CUDA memory leakage!\n",
    "        print(\"Done model: \", model)\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Completed repetition {i + 1}/{reps}\")\n",
    "\n",
    "df = pd.DataFrame(df_prot)\n",
    "df.to_csv(result_path)\n",
    "\n",
    "with open(f\"../res/models/confusion_matrices_ICL_{safe_name}_{round(time())}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(confusion_matrices, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment cell: compare generate-based extraction vs token-prob scoring\n",
    "# This cell implements a helper `choose_label_by_prob` and runs a small test across models\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _encode_labels(tokenizer, labels):\n",
    "    return [tokenizer.encode(l, add_special_tokens=False) for l in labels]\n",
    "\n",
    "@torch.no_grad()\n",
    "def choose_label_by_prob(model, tokenizer, prompt: str, labels: list[str], device: str = None):\n",
    "    \"\"\"Score candidate labels by computing log-probability of their token sequences.\"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    label_token_ids = _encode_labels(tokenizer, labels)\n",
    "    enc = tokenizer(prompt, return_tensors='pt')\n",
    "    input_ids = enc['input_ids'].to(device)\n",
    "    attention_mask = enc.get('attention_mask', None)\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "    scores = []\n",
    "    for tokens in label_token_ids:\n",
    "        cur_input = input_ids.clone()\n",
    "        cur_attn = attention_mask.clone() if attention_mask is not None else None\n",
    "        logprob = 0.0\n",
    "        valid = True\n",
    "        for t in tokens:\n",
    "            outputs = model(input_ids=cur_input, attention_mask=cur_attn)\n",
    "            logits = outputs.logits\n",
    "            last_logits = logits[0, -1, :]\n",
    "            probs = F.log_softmax(last_logits, dim=-1)\n",
    "            logp_token = probs[t].item()\n",
    "            logprob += logp_token\n",
    "            # append token to input and continue scoring\n",
    "            new_token = torch.tensor([[t]], device=device)\n",
    "            cur_input = torch.cat([cur_input, new_token], dim=1)\n",
    "            if cur_attn is not None:\n",
    "                cur_attn = torch.cat([cur_attn, torch.ones((1,1), device=device)], dim=1)\n",
    "        if not valid:\n",
    "            scores.append(float('-inf'))\n",
    "        else:\n",
    "            scores.append(logprob)\n",
    "    best_idx = int(torch.tensor(scores).argmax().item())\n",
    "    return labels[best_idx], scores\n",
    "\n",
    "def predict_and_eval(model_name: str, base_prompt: str, X: list, y: list, labels: list[str], shots_n: int = 1):\n",
    "    # load tokenizer and model (fallback to cpu if needed)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map='auto')\n",
    "    except Exception as e:\n",
    "        print('Falling back to CPU load for', model_name, '->', e)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32, device_map={'': 'cpu'})\n",
    "    device = model.device\n",
    "\n",
    "    y_gen = []\n",
    "    y_score = []\n",
    "    for i, text in enumerate(X):\n",
    "        prompt = base_prompt\n",
    "        if shots_n > 0:\n",
    "            # use first `shots_n` from X as examples (simple deterministic choice)\n",
    "            for j in range(shots_n):\n",
    "                prompt += inject_example(X[j], y[j])\n",
    "        prompt += inject_sample(text)\n",
    "        # generation-based prediction\n",
    "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=5, do_sample=False, eos_token_id=tokenizer.eos_token_id)\n",
    "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        gen_part = result[len(prompt):].strip() if result.startswith(prompt) else result\n",
    "        y_gen.append(extract_label(gen_part, labels))\n",
    "        # scoring-based prediction\n",
    "        best_label, scores = choose_label_by_prob(model, tokenizer, prompt, labels, device=str(device))\n",
    "        y_score.append(best_label)\n",
    "\n",
    "    # metrics\n",
    "    gen_acc = accuracy_score(y, y_gen)\n",
    "    gen_f1 = f1_score(y, y_gen, average='macro', zero_division=0)\n",
    "    score_acc = accuracy_score(y, y_score)\n",
    "    score_f1 = f1_score(y, y_score, average='macro', zero_division=0)\n",
    "    return { 'gen_acc': gen_acc, 'gen_f1': gen_f1 }, { 'score_acc': score_acc, 'score_f1': score_f1 }, y_gen, y_score\n",
    "\n",
    "\n",
    "prompt = \"\"\"Classify the sentiment of the tweet.\n",
    "Choose only one label: positive, negative, neutral.\n",
    "Reply ONLY with the label. Do not explain.\n",
    "\n",
    "\"\"\"\n",
    "# Run a small comparison across small models (adjust samples and shots as needed)\n",
    "models_to_test = ['facebook/opt-125m', 'facebook/opt-350m']\n",
    "labels = ['positive', 'negative', 'neutral']\n",
    "samples_for_test = 40\n",
    "# load samples (shots + test). We'll use shots_n=1 and the rest for evaluation\n",
    "X_all, y_all = load_train_dataset(ds_path, samples=samples_for_test + 1)\n",
    "X_test = X_all[1:]\n",
    "y_test = y_all[1:]\n",
    "shots_n = 1\n",
    "\n",
    "for m in models_to_test:\n",
    "    print('Testing model', m)\n",
    "    try:\n",
    "        gen_metrics, score_metrics, y_gen, y_score = predict_and_eval(m, prompt, X_test, y_test, labels, shots_n=shots_n)\n",
    "    except Exception as e:\n",
    "        print('Error testing', m, e)\n",
    "        continue\n",
    "    print(f'Model: {m} | Generate -> acc: {gen_metrics[\"gen_acc\"]:.3f}, f1: {gen_metrics[\"gen_f1\"]:.3f} | Score -> acc: {score_metrics[\"score_acc\"]:.3f}, f1: {score_metrics[\"score_f1\"]:.3f}')\n",
    "\n",
    "# End of experiment cell"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
