{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:31:01.212440Z",
     "start_time": "2025-12-05T11:31:01.209548Z"
    },
    "id": "heHwQb5io9IH"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:31:08.034991Z",
     "start_time": "2025-12-05T11:31:06.104923Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "LehkNYcSmX16",
    "outputId": "43796e6b-c8ae-452a-fc9b-e3b0d62afebb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/szymon/.cache/kagglehub/datasets/abhi8923shriv/sentiment-analysis-dataset/versions/9/train.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/szymon/.cache/kagglehub/datasets/abhi8923shriv/sentiment-analysis-dataset/versions/9/train.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download dataset from Kaggle\n",
    "ds_path = kagglehub.dataset_download(\"abhi8923shriv/sentiment-analysis-dataset\", path=\"train.csv\")\n",
    "print(\"Path to dataset files:\", ds_path)\n",
    "ds_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWOe67KZpbO_"
   },
   "source": [
    "### Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:31:20.046350Z",
     "start_time": "2025-12-05T11:31:20.042633Z"
    },
    "id": "mnCB7USZpXK4"
   },
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize tweet text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw tweet text\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text with URLs, HTML, mentions removed\n",
    "\n",
    "    Example:\n",
    "        >>> clean_text(\"@user I LOVE this! #amazing\")\n",
    "        'i love this! amazing'\n",
    "\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#(\\w+)\", r\"\\1\", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "\n",
    "def load_train_dataset(\n",
    "    path: str,\n",
    "    encoding: str = \"latin1\",\n",
    "    samples: int = 512,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Load and preprocess the sentiment dataset.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the CSV file\n",
    "        encoding (str): File encoding (default: 'latin1')\n",
    "        samples (int): Number of samples to randomly select\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X, y) where X is text array and y is label array\n",
    "\n",
    "    Example:\n",
    "        >>> X, y = load_train_dataset(\"data.csv\", samples=100)\n",
    "        >>> print(X[0], y[0])\n",
    "        'i love this product' 'positive'\n",
    "\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, encoding=encoding)\n",
    "    df = df.dropna()\n",
    "\n",
    "    df[\"preprocessed_text\"] = df[\"selected_text\"].apply(clean_text)\n",
    "\n",
    "    idxes = random.sample(range(df.shape[0]), samples)\n",
    "    return df[\"preprocessed_text\"].values[idxes], df[\"sentiment\"].values[idxes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUba2yG36sSz"
   },
   "source": [
    "### Prompt Engineering Functions for ICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:31:24.872628Z",
     "start_time": "2025-12-05T11:31:24.869972Z"
    },
    "id": "ev9eCXKp60vJ"
   },
   "outputs": [],
   "source": [
    "def inject_example(text: str, sentiment: str) -> str:\n",
    "    return f\"Tweet: {text}\\nSentiment: {sentiment}\\n\\n\"\n",
    "\n",
    "\n",
    "def inject_sample(text: str) -> str:\n",
    "    return (\n",
    "        f\"### Now classify the following:\\n\"\n",
    "        f\"Tweet: {text}\\nSentiment:\"\n",
    "    )\n",
    "\n",
    "def inject_sample_without_header(text: str) -> str:\n",
    "    return (\n",
    "        f\"Tweet: {text}\\nSentiment:\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpQRfB317PO_"
   },
   "source": [
    "### Label Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:31:28.581136Z",
     "start_time": "2025-12-05T11:31:28.578195Z"
    },
    "id": "zh_If7w47TJQ"
   },
   "outputs": [],
   "source": [
    "def extract_label(result: str, labels: list[str]) -> str:\n",
    "\n",
    "    for lab in labels:\n",
    "        if lab in result.lower():\n",
    "            return lab\n",
    "    print(\"Could not extract label from result:\", result)\n",
    "    return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkCeMvXu7wi3"
   },
   "source": [
    "### IN-CONTEXT LEARNING EXPERIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:31:50.573406Z",
     "start_time": "2025-12-05T11:31:50.569358Z"
    },
    "id": "ZMOXfJ4K7ymf"
   },
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    base_prompt: str, X: np.ndarray, y: np.ndarray, model_name: str,\n",
    ") -> tuple[list, list]:\n",
    "    \"\"\"Run ICL classification experiment on a test set.\n",
    "\n",
    "    This function:\n",
    "    1. Loads the specified language model\n",
    "    2. For each test sample, constructs a full prompt\n",
    "    3. Generates a prediction using the model\n",
    "    4. Extracts the predicted label\n",
    "    5. Collects all predictions and ground truth labels\n",
    "\n",
    "    Args:\n",
    "        base_prompt (str): Base prompt with instructions and examples\n",
    "        X (np.ndarray): Array of text samples to classify\n",
    "        y (np.ndarray): Array of true labels\n",
    "        model_name (str): HuggingFace model identifier (e.g., 'facebook/opt-1.3b')\n",
    "\n",
    "    Returns:\n",
    "        tuple[list, list]: (predicted_labels, true_labels)\n",
    "\n",
    "    \"\"\"\n",
    "    labels = list(np.unique(y))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, dtype=torch.float16, device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for idx, x in enumerate(X):\n",
    "        prompt = base_prompt + inject_sample(x)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        pred = result.split()[-1]\n",
    "        pred = extract_label(pred, labels)\n",
    "\n",
    "        y_true.append(y[idx])\n",
    "        y_pred.append(pred)\n",
    "\n",
    "    return y_pred, y_true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context window size experiment in ICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ctx_window_size_experiment(\n",
    "    base_prompt: str,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    model_name: str,\n",
    "    ctx_size: int = 2048,\n",
    ") -> tuple[list, list]:\n",
    "    \"\"\"Run ICL classification experiment on a test set.\n",
    "\n",
    "    This function:\n",
    "    1. Loads the specified language model\n",
    "    2. For each test sample, appends it to prompt and truncates to context window size\n",
    "    3. Generates a prediction using the model\n",
    "    4. Extracts the predicted label\n",
    "    5. Collects all predictions and ground truth labels\n",
    "\n",
    "    Args:\n",
    "        base_prompt (str): Base prompt with instructions and examples\n",
    "        X (np.ndarray): Array of text samples to classify\n",
    "        y (np.ndarray): Array of true labels\n",
    "        model_name (str): HuggingFace model identifier (e.g., 'facebook/opt-1.3b')\n",
    "        ctx_size (int): Context window size to truncate prompts to\n",
    "\n",
    "    Returns:\n",
    "        tuple[list, list]: (predicted_labels, true_labels)\n",
    "\n",
    "    \"\"\"\n",
    "    labels = list(np.unique(y))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for idx, x in enumerate(X):\n",
    "        prompt = base_prompt + (inject_sample(x) if idx == 0 else inject_sample_without_header(x))\n",
    "        prompt = prompt[-ctx_size:]  # Truncate to context window size\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        pred = result.split()[-1]\n",
    "        pred = extract_label(pred, labels)\n",
    "\n",
    "        y_true.append(y[idx])\n",
    "        y_pred.append(pred)\n",
    "\n",
    "    return y_pred, y_true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYcp5GGZ8ItJ"
   },
   "source": [
    "###  Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "R-6srF-b8Q_s"
   },
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: str,\n",
    "    base_prompt: str,\n",
    "    X,\n",
    "    y,\n",
    "    shots_n: int = 3,\n",
    "    experiment_func=run_experiment,\n",
    "    experiment_func_kwargs=None,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Evaluate ICL performance with a specific number of examples (shots).\n",
    "\n",
    "    Workflow:\n",
    "    1. Load (samples + shots_n) data points\n",
    "    2. Use first shots_n samples as examples in the prompt\n",
    "    3. Use remaining samples as the test set\n",
    "    4. Run classification and compute metrics\n",
    "\n",
    "    Args:\n",
    "        model (str): HuggingFace model name\n",
    "        base_prompt (str): Base instruction prompt\n",
    "        path (str): Path to dataset CSV\n",
    "        encoding (str): File encoding\n",
    "        samples (int): Number of test samples\n",
    "        shots_n (int): Number of examples to include (0=zero-shot, 1=one-shot, etc.)\n",
    "        experiment_func (callable): Function to run the experiment\n",
    "        experiment_func_kwargs (dict): Additional kwargs for experiment_func\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float, float, float]: (accuracy, f1_score, precision, recall)\n",
    "\n",
    "    \"\"\"\n",
    "    if experiment_func_kwargs is None:\n",
    "        experiment_func_kwargs = {}\n",
    "\n",
    "    if shots_n > 0:\n",
    "        base_prompt += \"### Examples\\n\"\n",
    "\n",
    "    for i in range(shots_n):\n",
    "        base_prompt += inject_example(X[i], y[i])\n",
    "\n",
    "    X, y = X[shots_n:], y[shots_n:]\n",
    "\n",
    "    y_pred, y_true = experiment_func(base_prompt, X, y, model, **experiment_func_kwargs)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    precision = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return acc, f1, precision, recall, y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7_C4V158h69"
   },
   "source": [
    "# Eksperymnet ICL konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJrHq2Qy8daE",
    "outputId": "054e2bed-fdd6-46d7-f466-ee88cc19a28a"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Classify the sentiment of the tweet.\n",
    "Choose only one label: positive, negative, neutral.\n",
    "Reply ONLY with the label. Do not explain.\n",
    "\n",
    "\"\"\"\n",
    "shots = [0, 1, 3, 20]\n",
    "reps = 10\n",
    "samples_n = 300\n",
    "result_path = f\"result_{round(time())}.csv\"\n",
    "model_name = \"facebook/opt-2.7b\"\n",
    "safe_name = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "\n",
    "df_prot = {\n",
    "    \"lp\": [],\n",
    "}\n",
    "for shot in shots:\n",
    "    df_prot[f\"{shot}-shots f1\"] = []\n",
    "    df_prot[f\"{shot}-shots acc\"] = []\n",
    "    df_prot[f\"{shot}-shots precision\"] = []\n",
    "    df_prot[f\"{shot}-shots recall\"] = []\n",
    "\n",
    "\n",
    "confusion_matrices = {shot: [] for shot in shots}\n",
    "\n",
    "for i in range(reps):\n",
    "    df_prot[\"lp\"].append(i)\n",
    "    for shot in shots:\n",
    "        X, y = load_train_dataset(\n",
    "            ds_path,\n",
    "            \"latin1\",\n",
    "            samples=samples_n + shot,\n",
    "        )\n",
    "        acc, f1, precision, recall, y_true, y_pred = evaluate(\n",
    "            model_name,\n",
    "            prompt,\n",
    "            X,\n",
    "            y,\n",
    "            shots_n=shot,\n",
    "            experiment_func=run_experiment,\n",
    "        )\n",
    "        df_prot[f\"{shot}-shots f1\"].append(f1)\n",
    "        df_prot[f\"{shot}-shots acc\"].append(acc)\n",
    "        df_prot[f\"{shot}-shots precision\"].append(precision)\n",
    "        df_prot[f\"{shot}-shots recall\"].append(recall)\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[\"positive\", \"negative\", \"neutral\"])\n",
    "        confusion_matrices[shot].append(cm)\n",
    "    print(f\"Completed repetition {i + 1}/{reps}\")\n",
    "\n",
    "df = pd.DataFrame(df_prot)\n",
    "df.to_csv(result_path)\n",
    "\n",
    "with open(f\"../res/shots/results/confusion_matrices_ICL_{safe_name}_{round(time())}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(confusion_matrices, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model size comparasion (with 3-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Classify the sentiment of the tweet.\n",
    "Choose only one label: positive, negative, neutral.\n",
    "Reply ONLY with the label. Do not explain.\n",
    "\n",
    "\"\"\"\n",
    "shots = 3\n",
    "reps = 10\n",
    "samples_n = 300\n",
    "result_path = f\"../res/models/result_{round(time())}.csv\"\n",
    "\n",
    "if not os.path.exists(\"../res/models/\"):\n",
    "    os.makedirs(\"../res/models/\")\n",
    "\n",
    "models = [\n",
    "    \"facebook/opt-125m\",\n",
    "    \"facebook/opt-350m\",\n",
    "    \"facebook/opt-1.3b\",\n",
    "    \"facebook/opt-2.7b\",\n",
    "    # \"facebook/opt-6.7b\",   # Out of memory on 8GB GPU\n",
    "]\n",
    "safe_name = \"model_size_clf\"\n",
    "\n",
    "\n",
    "df_prot = {\n",
    "    \"lp\": [],\n",
    "}\n",
    "for model in models:\n",
    "    df_prot[f\"{model} f1\"] = []\n",
    "    df_prot[f\"{model} acc\"] = []\n",
    "    df_prot[f\"{model} precision\"] = []\n",
    "    df_prot[f\"{model} recall\"] = []\n",
    "\n",
    "\n",
    "confusion_matrices = {model: [] for model in models}\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "for i in range(reps):\n",
    "    df_prot[\"lp\"].append(i)\n",
    "    X, y = load_train_dataset(\n",
    "        ds_path, \"latin1\", samples=samples_n + shots,\n",
    "    )\n",
    "    for model in models:\n",
    "        acc, f1, precision, recall, y_true, y_pred = evaluate(\n",
    "            model,\n",
    "            prompt,\n",
    "            X,\n",
    "            y,\n",
    "            shots_n=shots,\n",
    "            experiment_func=run_experiment,\n",
    "        )\n",
    "        df_prot[f\"{model} f1\"].append(f1)\n",
    "        df_prot[f\"{model} acc\"].append(acc)\n",
    "        df_prot[f\"{model} precision\"].append(precision)\n",
    "        df_prot[f\"{model} recall\"].append(recall)\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[\"positive\", \"negative\", \"neutral\"])\n",
    "        confusion_matrices[model].append(cm)\n",
    "\n",
    "        # Clearn GPU memory after each model evaluation, CUDA memory leakage!\n",
    "        print(\"Done model: \", model)\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Completed repetition {i + 1}/{reps}\")\n",
    "\n",
    "df = pd.DataFrame(df_prot)\n",
    "df.to_csv(result_path)\n",
    "\n",
    "with open(f\"../res/models/confusion_matrices_ICL_{safe_name}_{round(time())}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(confusion_matrices, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTX window size test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed repetition 1/10\n",
      "Completed repetition 2/10\n",
      "Completed repetition 2/10\n",
      "Completed repetition 3/10\n",
      "Completed repetition 3/10\n",
      "Completed repetition 4/10\n",
      "Completed repetition 4/10\n",
      "Completed repetition 5/10\n",
      "Completed repetition 5/10\n",
      "Completed repetition 6/10\n",
      "Completed repetition 6/10\n",
      "Completed repetition 7/10\n",
      "Completed repetition 7/10\n",
      "Completed repetition 8/10\n",
      "Completed repetition 8/10\n",
      "Completed repetition 9/10\n",
      "Completed repetition 9/10\n",
      "Completed repetition 10/10\n",
      "Completed repetition 10/10\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Classify the sentiment of the tweet.\n",
    "Choose only one label: positive, negative, neutral.\n",
    "Reply ONLY with the label. Do not explain.\n",
    "\n",
    "\"\"\"\n",
    "shots = 3\n",
    "reps = 10\n",
    "samples_n = 500\n",
    "result_path = f\"../res/ctx_windows/result_{round(time())}.csv\"\n",
    "model_name = \"facebook/opt-2.7b\"\n",
    "ctx_windows_sizes = [128, 256, 512, 1024, 2048]\n",
    "safe_name = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "\n",
    "df_prot = {\n",
    "    \"lp\": [],\n",
    "}\n",
    "for ctx_window_size in ctx_windows_sizes:\n",
    "    df_prot[f\"{ctx_window_size} f1\"] = []\n",
    "    df_prot[f\"{ctx_window_size} acc\"] = []\n",
    "    df_prot[f\"{ctx_window_size} precision\"] = []\n",
    "    df_prot[f\"{ctx_window_size} recall\"] = []\n",
    "\n",
    "confusion_matrices = {ctx_window_size: [] for ctx_window_size in ctx_windows_sizes}\n",
    "\n",
    "for i in range(reps):\n",
    "    df_prot[\"lp\"].append(i)\n",
    "    X, y = load_train_dataset(\n",
    "            ds_path,\n",
    "            \"latin1\",\n",
    "            samples=samples_n + shots,\n",
    "        )\n",
    "    for ctx_window_size in ctx_windows_sizes:\n",
    "        acc, f1, precision, recall, y_true, y_pred = evaluate(\n",
    "            model_name,\n",
    "            prompt,\n",
    "            X,\n",
    "            y,\n",
    "            shots_n=shots,\n",
    "            experiment_func=run_ctx_window_size_experiment,\n",
    "            experiment_func_kwargs={\"ctx_size\": ctx_window_size},\n",
    "        )\n",
    "        df_prot[f\"{ctx_window_size} f1\"].append(f1)\n",
    "        df_prot[f\"{ctx_window_size} acc\"].append(acc)\n",
    "        df_prot[f\"{ctx_window_size} precision\"].append(precision)\n",
    "        df_prot[f\"{ctx_window_size} recall\"].append(recall)\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[\"positive\", \"negative\", \"neutral\"])\n",
    "        confusion_matrices[ctx_window_size].append(cm)\n",
    "    print(f\"Completed repetition {i + 1}/{reps}\")\n",
    "\n",
    "df = pd.DataFrame(df_prot)\n",
    "df.to_csv(result_path)\n",
    "\n",
    "with open(f\"../res/ctx_windows/confusion_matrices_ICL_{safe_name}_{round(time())}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(confusion_matrices, f)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
